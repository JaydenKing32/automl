# -*- coding: utf-8 -*-
"""efficientdet tutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KuAu8h8yjQQzGSW7bN7D_FzVJWsmZUVz

# EfficientDet Tutorial: inference, eval, and training



<table align="left"><td>
  <a target="_blank"  href="https://github.com/google/automl/blob/master/efficientdet/tutorial.ipynb">
    <img src="https://www.tensorflow.org/images/GitHub-Mark-32px.png" />View source on github
  </a>
</td><td>
  <a target="_blank"  href="https://colab.sandbox.google.com/github/google/automl/blob/master/efficientdet/tutorial.ipynb">
    <img width=32px src="https://www.tensorflow.org/images/colab_logo_32px.png" />Run in Google Colab</a>
</td></table>

# 0. Install and view graph.

## 0.1 Install package and download source code/image.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# #@title

import os
import sys
import subprocess
import PIL
import tensorflow.compat.v1 as tf

MODEL = 'efficientdet-d0'  #@param

def download(m):
  if m not in os.listdir():
    subprocess.run(['wget', f'https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/{m}.tar.gz'])
    subprocess.run(['tar', 'zxf', f'{m}.tar.gz'])
  ckpt_path = os.path.join(os.getcwd(), m)
  return ckpt_path

# Download checkpoint.
ckpt_path = download(MODEL)
print('Use model in {}'.format(ckpt_path))

# Prepare image and visualization settings.
image_url =  'https://user-images.githubusercontent.com/11736571/77320690-099af300-6d37-11ea-9d86-24f14dc2d540.png'#@param
image_name = 'img.png' #@param
subprocess.run(['wget', image_url, '-O', image_name])

img_path = os.path.join(os.getcwd(), image_name)

min_score_thresh = 0.35  #@param
max_boxes_to_draw = 200  #@param
line_thickness =   2#@param

# Get the largest of height/width and round to 128.
image_size = max(PIL.Image.open(img_path).size)

# subprocess.run(['pipenv', 'run', 'python3', 'model_inspect.py', '--runmode=bm', '--model_name=efficientdet-d4', '--hparams=mixed_precision=true'])

"""## 0.2 View graph in TensorBoard"""

# Commented out IPython magic to ensure Python compatibility.
# !python model_inspect.py --model_name={MODEL} --logdir=logs &> /dev/null
# %load_ext tensorboard
# %tensorboard --logdir logs

# """# 1. inference
#
# ## 1.1 Benchmark network latency
# There are two types of latency:
# network latency and end-to-end latency.
#
# *   network latency: from the first conv op to the network class and box prediction.
# *   end-to-end latency: from image preprocessing, network, to the final postprocessing to generate a annotated new image.
# """
#
# # benchmaak network latency
# !python model_inspect.py --runmode=bm --model_name=efficientdet-d4 --hparams="mixed_precision=true"
#
# # With colab + Tesla T4 GPU, here are the batch size 1 latency summary:
# # D0 (AP=33.5):  14.9ms,  FPS = 67.2   (batch size 8 FPS=)
# # D1 (AP=39.6):  22.7ms,  FPS = 44.1   (batch size 8 FPS=)
# # D2 (AP=43.0):  27.9ms,  FPS = 35.8   (batch size 8 FPS=)
# # D3 (AP=45.8):  48.1ms,  FPS = 20.8   (batch size 8 FPS=)
# # D4 (AP=49.4):  81.9ms,  FPS = 12.2   (batch size 8 FPS=)
#
# """## 1.2 Benchmark end-to-end latency"""
#
# # Benchmark end-to-end latency (: preprocess + network + posprocess).
# #
# # With colab + Tesla T4 GPU, here are the batch size 1 latency summary:
# # D0 (AP=33.5): 22.7ms,  FPS = 43.1   (batch size 4, FPS=)
# # D1 (AP=39.6): 34.3ms,  FPS = 29.2   (batch size 4, FPS=)
# # D2 (AP=43.0): 42.5ms,  FPS = 23.5   (batch size 4, FPS=)
# # D3 (AP=45.8): 64.8ms,  FPS = 15.4   (batch size 4, FPS=)
# # D4 (AP=49.4): 93.7ms,  FPS = 10.7   (batch size 4, FPS=)
#
# m = 'efficientdet-d4'  # @param
# batch_size = 1  # @param
# m_path = download(m)
#
# saved_model_dir = 'savedmodel'
# !rm -rf {saved_model_dir}
# !python model_inspect.py --runmode=saved_model --model_name={m} \
#   --ckpt_path={m_path} --saved_model_dir={saved_model_dir} \
#   --batch_size={batch_size}  --hparams="mixed_precision=true"
# !python model_inspect.py --runmode=saved_model_benchmark --model_name={m} \
#   --ckpt_path={m_path} --saved_model_dir={saved_model_dir}/{m}_frozen.pb \
#   --batch_size={batch_size}  --hparams="mixed_precision=true" --input_image=testdata/img1.jpg
#
"""## 1.3 Inference images.

---
"""

# # first export a saved model.
# saved_model_dir = 'savedmodel'
# subprocess.run(["rm", "-rf", saved_model_dir])
# subprocess.run(['pipenv', 'run', 'python3', 'model_inspect.py',
#   '--runmode=saved_model', f'--model_name={MODEL}', f'--ckpt_path={ckpt_path}', f'--saved_model_dir={saved_model_dir}'])
#
# # Then run saved_model_infer to do inference.
# # Notably: batch_size, image_size must be the same as when it is exported.
# serve_image_out = 'serve_image_out'
# subprocess.run(["mkdir", serve_image_out])
#
# subprocess.run(['pipenv', 'run', 'python3', 'model_inspect.py',
#   '--runmode=saved_model_infer', f'--saved_model_dir={saved_model_dir}', f'--model_name={MODEL}', '--input_image=testdata/img1.jpg',
#   f'--output_image_dir={serve_image_out}', f'--min_score_thresh={min_score_thresh}', f'--max_boxes_to_draw={max_boxes_to_draw}'])

# from IPython import display
# display.display(display.Image(os.path.join(serve_image_out, '0.jpg')))
#
# # In case you need to specify different image size or batch size or #boxes, then
# # you need to export a new saved model and run the inferernce.
#
# serve_image_out = 'serve_image_out'
# !mkdir {serve_image_out}
# saved_model_dir = 'savedmodel'
# !rm -rf {saved_model_dir}
#
# # Step 1: export model
# !python model_inspect.py --runmode=saved_model \
#   --model_name=efficientdet-d0 --ckpt_path=efficientdet-d0 \
#   --hparams="image_size=1920x1280" --saved_model_dir={saved_model_dir}
#
# # Step 2: do inference with saved model.
# !python model_inspect.py --runmode=saved_model_infer \
#   --model_name=efficientdet-d0 --saved_model_dir={saved_model_dir} \
#   --input_image=img.png --output_image_dir={serve_image_out} \
#   --min_score_thresh={min_score_thresh}  --max_boxes_to_draw={max_boxes_to_draw}
#
# from IPython import display
# display.display(display.Image(os.path.join(serve_image_out, '0.jpg')))
#
# """## 1.4 Inference video"""
#
# # step 0: download video
# video_url = 'https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/data/video480p.mov'  # @param
# !wget {video_url} -O input.mov
#
# # Step 1: export model
# saved_model_dir = 'savedmodel'
# !rm -rf {saved_model_dir}
#
# !python model_inspect.py --runmode=saved_model \
#   --model_name=efficientdet-d0 --ckpt_path=efficientdet-d0 \
#   --saved_model_dir={saved_model_dir} --hparams="mixed_precision=true"
#
# # Step 2: do inference with saved model using saved_model_video
# !python model_inspect.py --runmode=saved_model_video \
#   --model_name=efficientdet-d0   --ckpt_path=efficientdet-d0 \
#   --saved_model_dir={saved_model_dir} --hparams="mixed_precision=true" \
#   --input_video=input.mov --output_video=output.mov
# # Then you can view the output.mov
#
# """# 3. COCO evaluation
#
# ## 3.1 COCO evaluation on validation set.
# """
#
# if 'val2017' not in os.listdir():
#   !wget http://images.cocodataset.org/zips/val2017.zip
#   !wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip
#   !unzip -q val2017.zip
#   !unzip annotations_trainval2017.zip
#
#   !mkdir tfrecord
#   !PYTHONPATH=".:$PYTHONPATH"  python dataset/create_coco_tfrecord.py \
#       --image_dir=val2017 \
#       --caption_annotations_file=annotations/captions_val2017.json \
#       --output_file_prefix=tfrecord/val \
#       --num_shards=32
#
# # Evalute on validation set (takes about 10 mins for efficientdet-d0)
# !python main.py --mode=eval  \
#     --model_name={MODEL}  --model_dir={ckpt_path}  \
#     --validation_file_pattern=tfrecord/val*  \
#     --val_json_file=annotations/instances_val2017.json
#
# """## 3.2 COCO evaluation on test-dev."""
#
# # Eval on test-dev is slow (~40 mins), please be cautious.
# RUN_EXPENSIVE_TEST_DEV_EVAL = True  #@param
#
# if RUN_EXPENSIVE_TEST_DEV_EVAL == True:
#   !rm *.zip *.tar tfrecord/ val2017/   # Cleanup disk space
#   # Download and convert test-dev data.
#   if "test2017" not in os.listdir():
#     !wget http://images.cocodataset.org/zips/test2017.zip
#     !unzip -q test2017.zip
#     !wget http://images.cocodataset.org/annotations/image_info_test2017.zip
#     !unzip image_info_test2017.zip
#
#     !mkdir tfrecord
#     !PYTHONPATH=".:$PYTHONPATH"  python dataset/create_coco_tfrecord.py \
#           --image_dir=test2017 \
#           --image_info_file=annotations/image_info_test-dev2017.json \
#           --output_file_prefix=tfrecord/testdev \
#           --num_shards=32
#
#   # Evalute on validation set: non-empty testdev_dir is the key pararmeter.
#   # Also, test-dev has 20288 images rather than val 5000 images.
#   !mkdir testdev_output
#   !python main.py --mode=eval  \
#       --model_name={MODEL}  --model_dir={ckpt_path}  \
#       --validation_file_pattern=tfrecord/testdev*  \
#       --eval_batch_size=8  --eval_samples=20288 \
#       --testdev_dir='testdev_output'
#   !rm -rf test2017  # delete images to release disk space.
#   # Now you can submit testdev_output/detections_test-dev2017_test_results.json to
#   # coco server: https://competitions.codalab.org/competitions/20794#participate
#
# """# 4. Training EfficientDets on PASCAL.
#
# ## 4.1 Prepare data
# """
#
# # Get pascal voc 2012 trainval data
# import os
# if 'VOCdevkit' not in os.listdir():
#   !wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar
#   !tar xf VOCtrainval_11-May-2012.tar
#
#   !mkdir tfrecord
#   !PYTHONPATH=".:$PYTHONPATH"  python dataset/create_pascal_tfrecord.py  \
#     --data_dir=VOCdevkit --year=VOC2012  --output_path=tfrecord/pascal
#
# # Pascal has 5717 train images with 100 shards epoch, here we use a single shard
# # for demo, but users should use all shards pascal-*-of-00100.tfrecord.
# file_pattern = 'pascal-00000-of-00100.tfrecord'  # @param
# images_per_epoch = 57 * len(tf.io.gfile.glob('tfrecord/' + file_pattern))
# images_per_epoch = images_per_epoch // 8 * 8  # round to 64.
# print('images_per_epoch = {}'.format(images_per_epoch))
#
# """## 4.2 Train Pascal VOC 2012 from ImageNet checkpoint for Backbone."""
#
# # Train efficientdet from scratch with backbone checkpoint.
# backbone_name = {
#     'efficientdet-d0': 'efficientnet-b0',
#     'efficientdet-d1': 'efficientnet-b1',
#     'efficientdet-d2': 'efficientnet-b2',
#     'efficientdet-d3': 'efficientnet-b3',
#     'efficientdet-d4': 'efficientnet-b4',
#     'efficientdet-d5': 'efficientnet-b5',
#     'efficientdet-d6': 'efficientnet-b6',
#     'efficientdet-d7': 'efficientnet-b6',
# }[MODEL]
#
#
# # generating train tfrecord is large, so we skip the execution here.
# import os
# if backbone_name not in os.listdir():
#   !wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientnet/ckptsaug/{backbone_name}.tar.gz
#   !tar xf {backbone_name}.tar.gz
#
# !mkdir /tmp/model_dir
# # key option: use --backbone_ckpt rather than --ckpt.
# # Don't use ema since we only train a few steps.
# !python main.py --mode=train_and_eval \
#     --training_file_pattern=tfrecord/{file_pattern} \
#     --validation_file_pattern=tfrecord/{file_pattern} \
#     --model_name={MODEL} \
#     --model_dir=/tmp/model_dir/{MODEL}-scratch  \
#     --backbone_ckpt={backbone_name} \
#     --train_batch_size=4 \
#     --eval_batch_size=4 --eval_samples={images_per_epoch}  \
#     --num_examples_per_epoch={images_per_epoch}  --num_epochs=1  \
#     --hparams="num_classes=20,moving_average_decay=0,mixed_precision=true"
#
# """## 4.3 Train Pascal VOC 2012 from COCO checkpoint for the whole net."""
#
# # generating train tfrecord is large, so we skip the execution here.
# import os
# if MODEL not in os.listdir():
#   !wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/{MODEL}.tar.gz
#   !tar xf {MODEL}.tar.gz
#
# !mkdir /tmp/model_dir/
# # key option: use --ckpt rather than --backbone_ckpt.
# !python main.py --mode=train_and_eval \
#     --training_file_pattern=tfrecord/{file_pattern} \
#     --validation_file_pattern=tfrecord/{file_pattern} \
#     --model_name={MODEL} \
#     --model_dir=/tmp/model_dir/{MODEL}-finetune \
#     --ckpt={MODEL} \
#     --train_batch_size=8 \
#     --eval_batch_size=8 --eval_samples={images_per_epoch}  \
#     --num_examples_per_epoch={images_per_epoch}  --num_epochs=1  \
#     --hparams="num_classes=20,moving_average_decay=0,mixed_precision=true"

"""## 4.4 View tensorboard for loss and accuracy."""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir /tmp/model_dir/
# Notably, this is just a demo with almost zero accuracy due to very limited
# training steps, but we can see finetuning has smaller loss than training
# from scratch at the begining.
